% Тут используется класс, установленный на сервере Papeeria. На случай, если
% текст понадобится редактировать где-то в другом месте, рядом лежит файл matmex-diploma-custom.cls
% который в момент своего создания был идентичен классу, установленному на сервере.
% Для того, чтобы им воспользоваться, замените matmex-diploma на matmex-diploma-custom
% Если вы работаете исключительно в Papeeria то мы настоятельно рекомендуем пользоваться
% классом matmex-diploma, поскольку он будет автоматически обновляться по мере внесения корректив
%

% По умолчанию используется шрифт 14 размера. Если нужен 12-й шрифт, уберите опцию [14pt]
%\documentclass[14pt]{matmex-diploma-custom}
\documentclass{matmex-diploma}

\begin{document}
% Год, город, название университета и факультета предопределены,
% но можно и поменять.
% Если англоязычная титульная страница не нужна, то ее можно просто удалить.
\filltitle{ru}{
    chair              = {Кафедра информационно-аналитических систем},
    title              = {Рекомендательная система музыки},
    % Здесь указывается тип работы. Возможные значения:
    %   coursework - Курсовая работа
    %   diploma - Диплом специалиста
    %   master - Диплом магистра
    %   bachelor - Диплом бакалавра
    type               = {coursework},
    position           = {студента},
    group              = 341,
    author             = {Руденко Дмитрий Андреевич},
    supervisorPosition = {доцент},
    supervisor         = {Графеева Н.Г.},
    %reviewerPosition   = {ст. преп.},
    %reviewer           = {Привалов А.\,И.},
    %chairHeadPosition  = {д.\,ф.-м.\,н., профессор},
    %chairHead          = {Хунта К.\,Х.},
    university         = {Федеральное государственное бюджетное образовательное учреждение высшего образования «Санкт-Петербургский государственный университет»},
    faculty            = {Математико-механический факультет},
    city               = {Санкт-Петербург},
    year               = {2015}
}

\maketitle
\tableofcontents
% У введения нет номера главы
\section*{Введение}
В век интернет-технологий каждый человек имеет доступ к терабайтам аудиофайлов. И каждый хочет найти именно ту музыку, которая ему подходит. Но вручную перебирать тысячи песен очень трудоемко. Поэтому хотелось бы автоматизировать этот процесс. Но как? Уже существуют рекомендательные системы: например, last.fm или vk.com. Но они основаны на коллаборативной фильтрации. Я же сделал систему, которая строит рекомендательную модель именно на анализе самих аудиофайлов, которые пользователь отметил как понравившиеся или не понравившиеся.

\section{Постановка задачи}
Целью данной работы является реализация приложения, которое по готовому набору пользовательской музыки формировало бы систему, способную определять, понравится ли конкретный аудиофайл пользователю. Необходимо реализовать следующие компоненты:
\begin{itemize}
\item{Чтение пользовательского набора wav файлов}
\item{Обработка начальных данных}
\item{Построение рекомендательной модели на основе собранных данных}
\item{Выборка подходящих аудиофайлов из нового набора}
\end{itemize}

Дополнительные задачи: разобраться в инструментах Machine Learning и FFT на языке Python.

\section{Обзор существующих систем}
Существуют несколько систем по рекомендации музыки: например, last.fm, vk.com, spotify\cite{Spotify}. Имея неполный список предпочтений пользователя, их рекомендательные системы предсказывают, какая музыка понравится ему. Кроме того, существует дочернее предприятие Spotify The Echo Nest, которое предоставляет пользователям по всему миру огромную базу данных по анализу музыки. Но опять таки, в этой базе может не содержаться тех аудиофайлов, которые именно пользователь выбрал. 

\section{Решение задачи}
Для работы был выбран язык Python, так как на нем реализованы библиотеки NumPy и Sklearn, про которые будет рассказано далее.
 \subsection{Чтение пользовательского набора}
 Составляется два набора данных: аудиозаписи, которые нравятся пользователю и которые нет. Чтение реализовано с помощью стандартных библиотек Python: wave, struct и os.
 \subsection{Обработка начальных данных}
 Анализируемый файл представляется в виде временного ряда, который делится на N снимков, каждый из которых приводится в АЧХ при помощи FFT-преобразования. То есть, из временного ряда мы получаем набор амплитут разных частот (FFT преобразование выполняется с помощью библиотеки NumPy). Далее, для каждого такого снимка считаются следующие статистические величины: среднее значение, медиана, стандартное отклонение, скос (skewness) и крутизна (kurtosis). Соответственно, для каждой композиции после вычислений мы будем иметь 5 * N величин, которые формируют точки в 5N-мерном пространстве.
 \subsection{Построение рекомендательной модели}
 Далее нам требуется, чтобы система умела определять, к какому из классов ("true" или "false") принадлежит новая 5N - мерная точка. Это задача Machine Learning\cite{lectionML}: у нас имеется матрица "object - features", и имеется вектор ответов ("-1" или "1") для каждого из объектов, и по этим данным требуется построить функцию, максимально точно определяющую класс целевого объекта. Изначально была простая идея обобщающего прямоугольника: по каждой из координат у всех понравившихся пользователю композиций выбираются максимум и минимум, а далее проверяем, попадает ли каждая из координат у целевой композиции в промежуток между максимом и минимумом. Минус очевиден: какая-либо композиция, сильно отличающаяся от всех остальных (так называемый "выброс"), очень сильно снижает точность определения класса. Поэтому дальнейший упор был сделан на алгоритмы Machine Learning, которые реализованы в библиотеке Sklearn\cite{scikit-learn}. Для исследования были выбраны следующие алгоритмы: KNeighbors Classification ("KNC"), Logistic regression, Support vector classification ("SVC") и Gradient Boosting Classification ("GBC").\paragraph{}Задана обучающая выборка пар «объект-ответ» $$X^m = \{(x_1,y_1),\dots,(x_m,y_m)\}$$ $a(x)$ - искомая функция
 \begin{itemize}
  \item{
 KNeighbors Classification (k ближайших соседей): Задана метрика $\rho(x,x')$. Для произвольного объекта $u$   расположим объекты обучающей выборки $x_i$ в порядке возрастания расстояний до $u$:
$$\rho(u,x_{1; u}) \leq  \rho(u,x_{2; u}) \leq \cdots \leq \rho(u,x_{m; u})$$ где через $x_{i; u}$ обозначается тот объект обучающей выборки, который является $i$-м соседом объекта $u$. Аналогичное обозначение введём и для ответа на i-м соседе: $y_{i; u}$. Таким образом, $$ a(u) = \mathrm{arg}\max_{y\in Y} \sum_{i=1}^m \bigl[ y(x_{i; u})=y \bigr] w(i,u)$$ $$w(i,u) = [i\leq k]$$.
}

  \item{
  Logistic regression (логистическая регрессия): $$a(x) = \mathrm{sign}(\sum_{j=1}^n w_j f_j(x) - w_0)$$ $w_j$ - вес $j$- ого признака, $w_0$ - порог принятия решения, $w=(w_1,\ldots,w_n)$ - вектор весов. Поиск вектора весов по выборке $X^m$ и есть задача алгоритма. В logistic regression она решается следующим образом: $$\sum_{i=1}^m \ln(1 + \exp( -y_i \langle x_i,w )) \to \min_{w}$$.
}
  \item{
  Support vector classification (метод опорных векторов): Основная идея метода — перевод исходных векторов(точек) в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором(расстоянием от гиперплоскости до ближайшейшей точки) в этом пространстве.  Постановка задачи аналогична logistic regression, отличается лишь задача нахождения оптимального вектора весов $w$ и порога вхождения $w_0$: $$\sum_{i = 1}^m(1 - M_i(x, w_0))_+ + \frac{1}{2C}(\left \| w \right \|)^2 \to \min_{w; w_0}$$ $$M_i(x, w_0) = y_i(\langle x_i,w \rangle - w_0)$$ Если выборка линейно неразделима(нельзя построить гиперплоскость, которая не проходила бы ни через одну точку выборки), то скалярное произведение в приведённой выше формуле заменяется нелинейной функцией ядра (скалярным произведением в пространстве с большей размерностью). В этом пространстве уже может существовать оптимальная разделяющая гиперплоскость. Библиотека Scikit-learn предоставляет следущие стандартные функции ядра ($K(x,x')$): "linear" - $\langle x, x'\rangle$; "polynomial" - $(\gamma \langle x, x'\rangle + r)^d$; "rbf" - $\exp(-\gamma |x-x'|^2)$; "sigmoid" - $\tanh(\gamma \langle x,x'\rangle + r)$ ($r$, $d$, $\gamma$ - определяются пользователем).
}
  \item{
  Gradient boosting classification\cite{friedman2001greedy} (градиентный бустинг): Суть бустинга - построения композиции алгоритмов Mashine Learning, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов. Наша цель - построить итоговую модель в виде $$a(x) = \sum_{m=1}^{T} \gamma_m a_m(x)$$, где $a_m(x)$ - базовые классификаторы. Вид базовых классификаторов определяется пользователем. Алгоритм градиентного бустинга схож с алгоритмом градиентного спуска(откуда и пошло его название): на каждой итерации алгоритм строит новую модель, максимально аппроксимирующую антиградиент функции потерь, и добавляет её к результирующей моделе. Количество итераций равно количеству базовых классификаторов(определяется пользователем). Формально: $$ a_m = \mathrm{arg}\min_{a} \sum_{j=1}^N (\frac{\partial L(y_j, a_{m-1}(x_j))}{\partial a_{m-1}(x_j)} - a(x_j))^2$$ $$\gamma_m = \mathrm{arg}\min_{\gamma} \sum_{j=1}^N L(y_j, a_{m-1}(x_j) + \gamma*a_m(x_j))$$ Здесь $L(y, a(x))$ - функция ошибки(определяется пользователем). В библиотеке Scikit-learn GBC в качестве базовых классификаторов использует дерево решений, а в качестве стандартных функций ошибок предлагает пользователю "deviance" - $log(1 + exp(-2ya(x)))$ или "exponential" - $exp(-ya(x))$
 }
\end{itemize}

 \subsection{Выборка подходящих аудиофайлов из нового набора}
 Обрабатываем новый набор аналогично обработки начальных данных (собираем статистические данные) и для каждого из объектов запрашиваем у построенной модели ответ на вопрос: "К какому классу принадлежит этот объект?". Те из объектов, которые попадут в класс подходящих ("1"), и будут искомые
 
\section{Тестирование моделей}
 Для тестирование моделей был взят начальный набор, содержащий 73 wav файла, и целевой набор, содержащий 33 wav файла.  Тестирование проводилось с помощью перекрестной проверки (cross validation), которая также реализована в библиотеке Sklearn. Так как перекрестная проверка основывается только на начальном наборе данных, было дополнительно проведено тестирование работы алгоритмов на целевом наборе данных. В данной работе представлены результаты проверки только тех параметров, которые сильно влияют в данном контексте на результат. Для остальных параметров, которые влияют на результат малозначительно, оставлены стандартные показатели.\paragraph{} Здесь и далее N - количество снимков с аудиофайла.

\newpage
\subsection{Тестирование количества соседей KNС}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knn100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:knn100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knn1000.png}
\caption{N = 1000}
\label{ris:knn1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knn10000.png}
\caption{N = 10000}
\label{ris:knn10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Тестирование метрик KNC}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knm100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:knm100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knm1000.png}
\caption{N = 1000}
\label{ris:knm1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{knm10000.png}
\caption{N = 10000}
\label{ris:kn10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Тестирование ядер SVC}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{svck100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:svck100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{svck1000.png}
\caption{N = 1000}
\label{ris:svck1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{svck10000.png}
\caption{N = 10000}
\label{ris:svck10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Тестирование количества базовых классификаторов GBC}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbce100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:gbce100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbce1000.png}
\caption{N = 1000}
\label{ris:gbce1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbce10000.png}
\caption{N = 10000}
\label{ris:gbce10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Тестирование функции ошибок GBC}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbcl100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:gbcl100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbcl1000.png}
\caption{N = 1000}
\label{ris:gbcl1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{gbcl10000.png}
\caption{N = 10000}
\label{ris:gbcl10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Сравнительный тест алгоритмов}

\begin{figure}[h!]
\begin{center}
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{b100.png}
\caption{N = 100} %% подпись к рисунку
\label{ris:b100} %% метка рисунка для ссылки на него
\end{minipage}
\hfill 
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{b1000.png}
\caption{N = 1000}
\label{ris:b1000}
\end{minipage}
\vfill
\begin{minipage}[h]{0.49\textwidth}
\includegraphics[width=1.14\textwidth]{b10000.png}
\caption{N = 10000}
\label{ris:b10000}
\end{minipage}
\end{center}
\end{figure}

\newpage
\subsection{Тестирование на целевом наборе}

\begin{figure}[h!]
\begin{center}
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    N & SVC & KNC & LogReg & GBC \\ \hline
    100 & 75.8 & 78.8 & 63.6 & 72.7\\ \hline
    1000 &75.6 & 72.7 & 72.7 & 72.7\\ \hline
    10000 &75.6 & 81.8 & 75.6 & 84.8\\ \hline
    \end{tabular}
\end{center}
\end{figure}

% У заключения нет номера главы
\section*{Заключение}
В ходе работы было реализовано приложение, которое по готовому набору пользовательской музыки формирует систему, способную определять, понравится ли конкретный аудиофайл пользователю. 
Написаны методы для чтения и обработки аудиофайлов, построения рекомендательной модели, выборки подходящих аудиофайлов из нового набора на языке Python  с использованием библиотек NumPy и Sklearn.


\setmonofont[Mapping=tex-text]{CMU Typewriter Text}
\bibliographystyle{ugost2008ls}
\bibliography{diploma.bib}
\end{document}

